{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as oj\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import sem\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../tools')\n",
    "from vol_utils.utils import cwd, set_up_plotting\n",
    "plt = set_up_plotting()\n",
    "from exp_utils import _get_SV_estimates\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e2a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(_get_SV_estimates)\n",
    "methods = ['MC', 'Owen', 'Sobol', 'Stratified', 'KernelSHAP', '2-FAE-0', '2-FAE-2', '2-FAE-5', '2-FAE-100']\n",
    "shorthands = ['MC','Owen', 'Sobol', 'stratified', 'kernel', 'Ours (0)', 'Ours (2)', 'Ours (5)', 'Ours (100)']\n",
    "experiment_options = [\"IG\", \"vol\", \"feature\"]\n",
    "\n",
    "# choose which expreriment to run\n",
    "experiment_name = experiment_options[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37368c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(df, name, csv=True, markdown=False, latex=True):\n",
    "    if csv:\n",
    "        df.to_csv(name + '.csv')\n",
    "\n",
    "    if markdown:\n",
    "        with open(name + '.md', 'w') as md_file:\n",
    "            df.to_markdown(md_file, index=False)\n",
    "\n",
    "    if latex:\n",
    "        df.to_latex(name + '.latex', index=False)\n",
    "    return\n",
    "\n",
    "\n",
    "def get_overall_dfs(result, n_trials=5):\n",
    "\n",
    "    stored_dict = result['res'].item()\n",
    "    n = result['n']\n",
    "    exact_SV = stored_dict['Exact']\n",
    "    exact_SV /= np.mean(exact_SV)\n",
    "\n",
    "    overall_dfs = []\n",
    "    for t in range(n_trials):\n",
    "\n",
    "        desirability_data  = defaultdict(list)\n",
    "        mis_counts = defaultdict(float)\n",
    "        mis_sums = defaultdict(float)\n",
    "\n",
    "        afs_data = defaultdict(list)\n",
    "\n",
    "        mape_res = defaultdict(float)\n",
    "        mse_res = defaultdict(float)\n",
    "        pearsonr_res = defaultdict(float)\n",
    "        cosine_res = defaultdict(float)\n",
    "\n",
    "        acc_data = defaultdict(list)\n",
    "\n",
    "        for method, shorthand in zip(methods, shorthands):\n",
    "            mcs, afs, min_afs = stored_dict[method+'statistics'][t]\n",
    "            sv_estimates = _get_SV_estimates(n, mcs)\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if i == j: continue\n",
    "                    exact_diff = exact_SV[i] - exact_SV[j]\n",
    "                    est_diff = sv_estimates[i] - sv_estimates[j]\n",
    "\n",
    "                    mis_counts[method] += np.sign(exact_diff) != np.sign(est_diff)\n",
    "\n",
    "                    ref_SV = max(np.abs(exact_SV[i]), np.abs(exact_SV[j]))  \n",
    "                    mis_sums[method] += np.abs(exact_diff - est_diff) / ref_SV\n",
    "\n",
    "            cols = ['inversion counts', 'inversion error' ]        \n",
    "            desirability_data[shorthand] = [ mis_counts[method], mis_sums[method]]\n",
    "            desirability_df = pd.DataFrame(desirability_data, index=cols)\n",
    "            desirability_df.T.index.name= 'baselines'\n",
    "\n",
    "            cols = ['(max-min)/mean', 'std', 'nlnsw']\n",
    "\n",
    "            mcs, afs, min_afs = stored_dict[method+'statistics'][t]\n",
    "\n",
    "            normed_afs = afs\n",
    "            normed_afs = normed_afs / np.sum(normed_afs) * len(normed_afs)\n",
    "            nlnsw = - np.log(normed_afs).sum()\n",
    "                        \n",
    "            afs_data[shorthand] = [(np.max(afs) - np.min(afs)) / np.mean(afs), np.std(afs), nlnsw]\n",
    "            afs_df = pd.DataFrame(afs_data, index=cols)\n",
    "            afs_df.T.index.name= 'baselines'\n",
    "\n",
    "\n",
    "            cols = ['MAPE', 'MSE', 'Pearson', 'Cosine']\n",
    "\n",
    "            mape_res[method] = mean_absolute_percentage_error(exact_SV, sv_estimates)\n",
    "            mse_res[method] = mean_squared_error(exact_SV, sv_estimates)\n",
    "            pearsonr_res[method], pvalue = pearsonr(exact_SV, sv_estimates)\n",
    "            cosine_res[method] = cosine_similarity(exact_SV.reshape(1, -1), sv_estimates.reshape(1, -1)).squeeze()\n",
    "\n",
    "            acc_data[shorthand] = [ mape_res[method], mse_res[method], pearsonr_res[method], cosine_res[method]]\n",
    "\n",
    "            acc_df = pd.DataFrame(acc_data, index=cols)\n",
    "            acc_df[:2].T.index.name = 'baselines'\n",
    "            acc_df[:2].T\n",
    "\n",
    "        overall_df = acc_df[:2].T.merge(desirability_df.T, on='baselines').merge(afs_df.T, on='baselines')\n",
    "        overall_dfs.append(overall_df)\n",
    "    return overall_dfs\n",
    "\n",
    "def get_mean_sem_dfs(overall_dfs):\n",
    "\n",
    "    metrics = ['MAPE', 'MSE', 'inversion counts', 'inversion error', '(max-min)/mean', 'std','nlnsw' ]\n",
    "\n",
    "    np_arr = np.asarray([overall_df.values for overall_df in overall_dfs])\n",
    "    std_errs = np.zeros((len(methods), len(metrics)))\n",
    "    data_dict = {'baselines': shorthands}\n",
    "    for i in range(len(methods)):\n",
    "        for j in range(len(metrics)):\n",
    "            std_errs[i,j] = sem(np_arr[:, i, j ])\n",
    "\n",
    "            \n",
    "    for j, metric in enumerate(metrics):\n",
    "        data_dict[metric] = list(std_errs[:,j]) \n",
    "    sem_df = pd.DataFrame(data_dict, index=data_dict['baselines'])\n",
    "    \n",
    "    means = np.zeros((len(methods), len(metrics)))\n",
    "\n",
    "    for i in range(len(methods)):\n",
    "        for j in range(len(metrics)):\n",
    "            means[i,j] = np.mean(np_arr[:, i, j ])\n",
    "    data_dict = {'baselines': shorthands}\n",
    "\n",
    "    for j, metric in enumerate(metrics):\n",
    "        data_dict[metric] = list(means[:,j]) \n",
    "    mean_df = pd.DataFrame(data_dict, index=data_dict['baselines'])\n",
    "    \n",
    "    return mean_df, sem_df\n",
    "\n",
    "def get_max_se_mean_ratio(mean_df, sem_df):\n",
    "\n",
    "    m_values = mean_df.drop(['baselines'], axis=1).values\n",
    "    s_values = sem_df.drop(['baselines'], axis=1).values\n",
    "    max_ratio = 0\n",
    "    for i in range(len(methods)):\n",
    "        for j in range(6):\n",
    "            se = s_values[i, j]\n",
    "            avg = m_values[i, j]\n",
    "\n",
    "            if avg != 0:\n",
    "                ratio = se / avg\n",
    "                if max_ratio < ratio:\n",
    "                    max_ratio = ratio\n",
    "\n",
    "    return max_ratio\n",
    "\n",
    "\n",
    "def get_mean_se_str_df(mean_df, sem_df):\n",
    "    m_df = mean_df.drop(['baselines'], axis=1)\n",
    "    s_df = sem_df.drop(['baselines'], axis=1)\n",
    "\n",
    "    mean_se_data_dict = {'baselines': shorthands}\n",
    "\n",
    "    for metric in ['MAPE', 'MSE', 'inversion counts', 'inversion error', 'nlnsw', '(max-min)/mean']:\n",
    "        res = []\n",
    "        for avg, se in zip(m_df[metric].round(5), s_df[metric].round(5)):\n",
    "\n",
    "            if 0.1<avg<10:\n",
    "                avg_str = f'{avg:.2f}'\n",
    "            else:\n",
    "                avg_str = f'{avg:.2e}'\n",
    "            if 0.1< se <100:\n",
    "                se_str = f'{se:.2f}'\n",
    "            else:\n",
    "                se_str = f'{se:.1e}'\n",
    "\n",
    "            res.append(f'{avg_str} ({se_str})' )\n",
    "        mean_se_data_dict[metric] = res\n",
    "    mean_se_df = pd.DataFrame(mean_se_data_dict)\n",
    "    mean_se_df.set_index('baselines')\n",
    "    \n",
    "    return mean_se_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b42fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 5\n",
    "PATH = '../experiment_data'\n",
    "results_dir = oj(PATH, experiment_name)\n",
    "\n",
    "with cwd(results_dir):\n",
    "    for result_dir in os.listdir():\n",
    "        try:\n",
    "            if result_dir.endswith('.npz'):\n",
    "                result = np.load(result_dir, allow_pickle=True)\n",
    "                overall_dfs = get_overall_dfs(result, n_trials=n_trials)\n",
    "                mean_df, sem_df = get_mean_sem_dfs(overall_dfs)\n",
    "                name = result_dir.replace('res_', '').replace('.npz', '')\n",
    "                mean_se_str_df = get_mean_se_str_df(mean_df, sem_df)\n",
    "                save(mean_se_str_df, name+'-mean-sem', csv=False)  \n",
    "        except Exception as e:\n",
    "            print(f'Exception is {e} with {result_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30825046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e9822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
